# ğŸ¦ Banking Modern Data Stack  
### **Snowflake â€¢ dbt â€¢ Apache Airflow â€¢ Kafka â€¢ Debezium â€¢ Python â€¢ Docker â€¢ GitHub CI/CD**

Welcome to a fully engineered **endâ€‘toâ€‘end banking data platform** designed to mirror real production data ecosystems used by modern financial institutions.

This project brings together streaming, warehousing, orchestration, transformation, and CI/CD into one unified system â€” all powered by todayâ€™s most inâ€‘demand data engineering tools.

---

# ğŸš€ Project Summary

This repository demonstrates how raw operational banking data flows through a **complete data pipeline**, from generation all the way to curated analytics layers.

It includes:

- Synthetic OLTP banking data  
- Realâ€‘time CDC pipelines  
- Multiâ€‘layer Snowflake warehouse  
- dbt transformations + snapshots  
- Airflowâ€‘driven orchestration  
- Automated CI/CD for reliability  

The goal is to provide a handsâ€‘on, productionâ€‘style architecture suitable for demos, learning, and portfolio projects.

---

# ğŸ§± Highâ€‘Level Architecture

```
Data Generator â†’ Kafka + Debezium (CDC) â†’ MinIO (S3) â†’ Airflow â†’ Snowflake  
       â†“                   â†“                     â†“            â†“  
 Synthetic OLTP      Real-time changes      Bronze layer   dbt models  
```

**Pipeline Lifecycle**

1. **Synthetic Events** â€“ Banking records created using Python + Faker.  
2. **CDC Capture** â€“ Debezium monitors Postgres WAL logs and pushes events to Kafka topics.  
3. **Object Storage Landing Zone** â€“ Events written to MinIO (S3-compatible).  
4. **Airflow Jobs** â€“ Batch and incremental ingestion into Snowflake.  
5. **dbt Transformations** â€“ Standardized staging, dimensions, facts, and SCD2 snapshots.  
6. **CI/CD** â€“ Every PR triggers automated dbt tests, compilation, and validations.

---

# âš¡ Technologies Used

### **Compute & Warehousing**
- Snowflake (Cloud Data Warehouse)  
- dbt (Transformations, testing, documentation)

### **Data Movement & Streaming**
- Apache Kafka  
- Debezium (CDC from Postgres WAL)  
- MinIO (S3-compatible object storage)

### **Orchestration**
- Apache Airflow (DAG scheduling, ingestion, and automation)

### **Development & Deployment**
- Docker / Docker Compose  
- Python (data generation, utility scripts)  
- GitHub Actions (CI/CD)

---

# ğŸ“‚ Repository Layout

```
banking-modern-datastack/
â”‚
â”œâ”€â”€ .github/workflows/        # CI/CD pipelines
â”œâ”€â”€ banking_dbt/              # dbt project (models, marts, snapshots)
â”œâ”€â”€ consumer/                 # Kafka-to-MinIO ingestion
â”œâ”€â”€ data-generator/           # Synthetic data creation
â”œâ”€â”€ docker/                   # Airflow configs, DAGs
â”œâ”€â”€ kafka-debezium/           # CDC connectors for Postgres
â”œâ”€â”€ postgres/                 # OLTP schema + seed scripts
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# ğŸ” Key Features

### âœ” Realistic Banking OLTP Simulation  
Customers, accounts, transactions â€” all generated with proper relationships and constraints.

### âœ” Real-Time CDC Pipeline  
Debezium monitors Postgres and streams every insert/update/delete into Kafka.

### âœ” S3â€‘Style Raw Zone  
Kafka events land in MinIO buckets for downstream batch or microâ€‘batch ingestion.

### âœ” Airflowâ€‘Driven ETL  
Automated pipelines move data into Snowflakeâ€™s **Bronze â†’ Silver â†’ Gold** layers.

### âœ” dbt Transformations & Snapshots  
- Clean staging models  
- Dimensional models & fact tables  
- Slowly Changing Dimensions (SCD2) via dbt snapshots

### âœ” CI/CD Integration  
GitHub Actions validate dbt models, run tests, and ensure every change remains productionâ€‘ready.

---

# ğŸ¯ Final Outcomes

By the end of pipeline execution, you will have:

- Fully automated data ingestion from OLTP â†’ Warehouse  
- Realâ€‘time CDC events captured and stored reliably  
- Cleaned, conformed analytics tables ready for dashboards  
- Historical tracking of customer & account changes  
- A complete modern data stack setup runnable locally via Docker  

Perfect for:
- Data engineering portfolios  
- Demoing a realâ€‘world architecture  
- Learning CI/CD, dbt, CDC, and orchestration  
- Showcasing Snowflake + modern tooling in action  

---



